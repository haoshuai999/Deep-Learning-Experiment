# Experiment on Parameters
## 3a When using different activation functions, what differences do you observe, and why?
The regular ReLu function might be inactive and only output 0 in some neural network because it always outputs 0 for negative inputs, so I implemented the leaky ReLu function as my activation function to prevent neurons from dying. I observed that the leaky ReLu activation function (about 95 percent accuracy) performed better than the Tanh function (about 94 percent accuracy). And the Sigmoid activation function gave out the worst accuracy (about 91 percent accuracy). Besides, the loss of the training samples went below 0.20 in the seventh epoch when using the leaky ReLu function. While the losses for the other two models were above 0.20 in the tenth epoch. The leaky ReLu function performed better because it is sensitive to all input values. However, from the graph of the Sigmoid and Tanh functions, we can tell that when the input value is very positive or negative, the output is closer to each other. Only when the input is closer to zero, the output would be reasonable. So, the ReLu activation function doesn't suffer from the vanishing gradient problem and has higher prediction accuracy.

## 3b Do optimizers like Momentum or Adam really make a difference? How about different weight initialize strategies?
Yes, there are differences. When using a neural network to search for optimal solutions, we can only use Non-Deterministic Algorithms to get a relatively good solution. To prevent the algorithm from stuck in an area when performing gradient descent, we want to introduce some randomness. \
The Adam optimizer (about 97.5 percent accuracy with Glorot Uniform) works better than the Momentum optimizer (about 95 percent accuracy with Momentum), although both algorithms use randomness during the progression of the search. I think this is because the SGD with momentum optimizer accumulates the gradient of the past steps to determine the direction to go, which reduces the time to convergence comparing to regular SGD optimizer. However, the Adam optimizer takes one more step further by applying momentum and computing individual adaptive learning rates for different parameters. \
To test the performance of different initializer, I used the zeros initializer as a control group, because it doesn't introduce any randomness by setting all the initial weights as zero and the algorithm stuck on a platform without improving. The Glorot Uniform initializer (about 98 percent accuracy using Adam) usually works slightly better than the Random Normal initializer (about 97.7 percent accuracy using adam) using testing. I think it is because the Random Normal initializer just generates tensors with a normal distribution, which help increase the accuracy but doesn't control the range of randomness. The Glorot Uniform initializer sets a limit for the randomness based on the number of input and output units, which slightly increases the accuracy. However, when re-running the model, sometimes Random Normal initializer performs better, which means for a simple training set like MNIST, these two initializers don't have a lot of differences.

## 3c Demonstrate the vanishing gradient problem.
I first built a thirteen-layer DNN model and used Sigmoid as the activation function (include one softmax layer). After running ten epochs, I found that the accuracy of the Sigmoid model is only about 10 percent. So I changed my activation function to ReLu. Again, I ran ten epochs and the accuracy is about 97 percent. From the histogram of tensorboard, I noticed that the Sigmoid model's gradients gradually turned to zero (vanished) in each epoch, and the shape of activation histograms remained the same because the outputs of each dense layer didn't change a lot. In my second experiment, the ReLu model doesn't have the vanishing gradient problem. The Sigmoid model's gradients are much larger or smaller than zero and the shape of activation histograms changed by epoch.