# Experiment on Parameters
## 3a
The regular ReLu function might be inactive and only output 0 in some neural network because it always outputs 0 for negative inputs, so I implemented the leaky ReLu function as my activation function to prevent neurons from dying. I observed that the leaky ReLu activation function (95 percent accuracy) performed better than the Tanh function (94 percent accuracy). And the Sigmoid activation function gave out the worst accuracy (91 percent accuracy). Besides, the loss of the training samples went below 0.20 in the seventh epoch when using the leaky ReLu function. While the losses for the other two models were above 0.20 in the tenth epoch. The leaky ReLu function performed better because it is sensitive to all input values. However, from the graph of the Sigmoid and Tanh functions, we can tell that when the input value is very positive or negative, the output is closer to each other. Only when the input is closer to zero, the output would be reasonable. So, the ReLu activation function doesn't suffer from the vanishing gradient problem and has higher prediction accuracy.

## 3b
Yes, there are differences. When using a neural network to search for optimal solutions, we can only use Non-Deterministic Algorithms to get a relatively good solution. To prevent the algorithm from stuck in an area when performing gradient descent, we want to introduce some randomness. \
The Adam optimizer (97.47 percent accuracy with Glorot Uniform) works better than the Momentum optimizer (95.33 percent accuracy with Momentum), although both algorithms use randomness during the progression of the search. I think this is because the SGD with momentum optimizer accumulates the gradient of the past steps to determine the direction to go, which reduces the time to convergence comparing to regular SGD optimizer. However, the Adam optimizer takes one more step further by applying momentum and computing individual adaptive learning rates for different parameters. \
To test the performance of different initializer, I used the zeros initializer as a control group, because it doesn't introduce any randomness by setting all the initial weights as zero and the algorithm stuck on a platform without improving. The Glorot Uniform initializer (97.47 percent accuracy using Adam) works slightly better than the Random Normal initializer (97.44 percent accuracy using adam). I think it is because the Random Normal initializer just generates tensors with a normal distribution, which help increase the accuracy but doesn't control the range of randomness. The Glorot Uniform initializer sets a limit for the randomness based on the number of input and output units, which slightly increases the accuracy. However, when re-running the model, sometimes Random Normal initializer performs better, which means for a simple training set like MNIST, these two initializers don't have a lot of differences.

## 3c
I first built a thirteen-layer DNN model and used Sigmoid as the activation function. After running two epochs, I found that the accuracy of the model is only about 10 percent. So I changed my activation function to ReLu. Again, I ran two epochs and the accuracy is about 96 percent. From the histogram of tensorboard, I noticed that the Sigmoid model's gradients are close to zero in the first few layers, but the ReLu model doesn't have the problem.